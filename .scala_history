pyspark.cmd
spark-shell.cmd
java -version
sc.version
sc.appName
:quit
spark.range(1).withColumn("status", lit("All seems fine. Congratulations!")).show(false)
val linhas = sc.textFile("C:/spark/spark/README.md")
linhas.count()
linhas.first()
cal linhasCluster = linhas.filter(line => line.contains("cluster"))
val linhasCluster = linhas.filter(line => line.contains("cluster"))
linhasCluster
linhasCluster.count()
linhasCluster.first()
val linhasSpark = linhas.filter(line => line.contains("Spark"))
linhasSpark.count()
val linhasClusterESpark = linhasCluster.union(linhasSpark)
linhasClusterESpark.count()
val entrada = sc.parallelize(List(1, 2, 4, 5, 6, 7, 8, 3, 0))
val resultado = entrada.map(x => x*x)
resultado.count()
resultado.take(9).foreach(print)
resultado.take(9).foreach(println)
linhasSpark.take(10).foreach(println)
import org.apache.spark.storage.StorageLevel
val nums = sc.parallelize(List(1,2,3,4,5,6,7,8))
val resultado = nums.map(x => x*x)
resultado.persist(StorageLevel.DISK_ONLY)
println(resultado.count)
println(resultado.count())
println(resultado.collect().mkString(","))
val cliente_Vendas = sc.parallelize(List("1 500", "2 700", "3 1000", "2 200", "1 800", "4 650"))
cliente_Vendas.count
val cliente_Vendas = sc.parallelize(List("1 500", "2 700", "1 300", "3 1000", "2 200", "1 800", "4 650"))
cliente_Vendas.count
cliente_Vendas.collect
val pares = cliente_Vendas.map(x => (x.split(" ")(0), x))
pares.count
pares.collect
val pares = cliente_Vendas.map(x => (x.split(" ")(0), x.split(" ")(1)))
pares.collect
val cliente_Vendas = sc.parallelize(Array( ("Abel", 500), ("Bel", 700), ("Abel", 300))
val cliente_Vendas = sc.parallelize(Array( ("Abel", 500), ("Bel", 700), ("Abel", 300)))
val aux = cliente_Vendas.reduceByKey((x,y) => (x+y))
aux.collect
val a = sc.parallelize(("Abel", 500))
